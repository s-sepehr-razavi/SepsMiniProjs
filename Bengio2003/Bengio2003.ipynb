{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, n, m, h, l):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.fc1 = nn.Linear(l, m)\n",
    "        self.fc2 = nn.Linear(n * m, h, bias=True)\n",
    "        self.fc3 = nn.Linear(h, l, bias=True)\n",
    "\n",
    "    def forward(self, x): # n * l\n",
    "        x = self.fc1(x) # n * m\n",
    "        x = x.reshape(-1, self.n * self.m) # 1 * nm\n",
    "        x = self.fc2(x) \n",
    "        x = nn.functional.tanh(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = nn.functional.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "persian_chars_list = \"آ ا ب پ ت ث ج چ ح خ د ذ ر ز ژ س ش ص ض ط ظ ع غ ف ق ک گ ل م ن و ه ی\".split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    \n",
    "    def _omit_unnecessary_chars(self, string, chars_list):    \n",
    "        str = \"\"\n",
    "        for ch in string:\n",
    "            if ch in chars_list:\n",
    "                str += ch\n",
    "        return str\n",
    "\n",
    "\n",
    "    def _omit_parantheses(self, string, char):\n",
    "        index = string.find(char)\n",
    "        if index == -1:\n",
    "            return string\n",
    "        return string[:index]\n",
    "        \n",
    "\n",
    "    def clean_dataset(self, strings, chars_list):\n",
    "        arr = []\n",
    "        for i in range(len(strings)):\n",
    "            s = self._omit_parantheses(strings[i], ' ')\n",
    "            s = self._omit_parantheses(s, '(')\n",
    "            s = self._omit_parantheses(s, '[')\n",
    "            s = self._omit_unnecessary_chars(s, chars_list)\n",
    "            if len(s) > 1:\n",
    "                arr.append(s)\n",
    "        return arr\n",
    "    \n",
    "    def generate_dicts(self, chars_list):\n",
    "        char_int = {}\n",
    "        int_char = {}\n",
    "        n = len(chars_list)\n",
    "        for i in range(n):\n",
    "            char_int[chars_list[i]] = i + 1\n",
    "            int_char[i + 1] = chars_list[i]\n",
    "        char_int['.'] = 0\n",
    "        int_char[0] = '.'\n",
    "        char_int['\\u200c'] = n+1\n",
    "        int_char[n+1] = '\\u200c'\n",
    "        return (char_int, int_char)\n",
    "    \n",
    "    def generate_data_set(self, block_size, path, chars_list):        \n",
    "        X, Y = [], []\n",
    "        names = self.clean_dataset(path, chars_list)\n",
    "        char_int = self.generate_dicts(chars_list)[0]       \n",
    "\n",
    "        for w in names:            \n",
    "            context = [0] * block_size\n",
    "            for ch in w + '.':                \n",
    "                ix = char_int[ch]\n",
    "                X.append(context)\n",
    "                Y.append(ix)                \n",
    "                context = context[1:] + [ix]\n",
    "        X = torch.tensor(X).to(device=self.device)\n",
    "        # print(X.max())\n",
    "        X = F.one_hot(X, num_classes=len(chars_list) + 1).float()\n",
    "        Y = torch.tensor(Y).to(device=self.device)\n",
    "\n",
    "        return (X,Y)\n",
    "    \n",
    "\n",
    "    def __init__(self, path, chars_list, block_size, device) -> None:\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            names = file.read().splitlines()\n",
    "        \n",
    "        self.device = device\n",
    "        self.X, self.Y = self.generate_data_set(block_size, names, chars_list)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        print(x)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # print(loss)\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "loss: 3.551260  [   32/ 4380]\n",
      "loss: 2.243391  [ 3232/ 4380]\n",
      "loss: 2.700101  [   32/ 4380]\n",
      "loss: 2.196141  [ 3232/ 4380]\n",
      "loss: 2.723104  [   32/ 4380]\n",
      "loss: 2.115409  [ 3232/ 4380]\n",
      "loss: 2.571261  [   32/ 4380]\n",
      "loss: 2.180636  [ 3232/ 4380]\n",
      "loss: 2.544547  [   32/ 4380]\n",
      "loss: 2.199903  [ 3232/ 4380]\n",
      "loss: 2.481555  [   32/ 4380]\n",
      "loss: 2.086476  [ 3232/ 4380]\n",
      "loss: 2.355093  [   32/ 4380]\n",
      "loss: 2.145050  [ 3232/ 4380]\n",
      "loss: 2.592844  [   32/ 4380]\n",
      "loss: 2.110377  [ 3232/ 4380]\n",
      "loss: 2.230784  [   32/ 4380]\n",
      "loss: 2.060028  [ 3232/ 4380]\n",
      "loss: 2.298032  [   32/ 4380]\n",
      "loss: 2.139713  [ 3232/ 4380]\n",
      "loss: 2.434499  [   32/ 4380]\n",
      "loss: 2.055780  [ 3232/ 4380]\n",
      "loss: 2.235286  [   32/ 4380]\n",
      "loss: 1.954289  [ 3232/ 4380]\n",
      "loss: 2.275639  [   32/ 4380]\n",
      "loss: 2.008130  [ 3232/ 4380]\n",
      "loss: 2.283372  [   32/ 4380]\n",
      "loss: 2.033384  [ 3232/ 4380]\n",
      "loss: 2.312065  [   32/ 4380]\n",
      "loss: 1.974115  [ 3232/ 4380]\n",
      "loss: 2.149675  [   32/ 4380]\n",
      "loss: 2.063908  [ 3232/ 4380]\n",
      "loss: 2.383432  [   32/ 4380]\n",
      "loss: 1.942889  [ 3232/ 4380]\n",
      "loss: 2.175824  [   32/ 4380]\n",
      "loss: 1.905617  [ 3232/ 4380]\n",
      "loss: 2.182502  [   32/ 4380]\n",
      "loss: 2.056911  [ 3232/ 4380]\n",
      "loss: 2.330273  [   32/ 4380]\n",
      "loss: 1.964927  [ 3232/ 4380]\n",
      "loss: 2.356657  [   32/ 4380]\n",
      "loss: 1.951360  [ 3232/ 4380]\n",
      "loss: 2.276892  [   32/ 4380]\n",
      "loss: 2.029062  [ 3232/ 4380]\n",
      "loss: 2.167718  [   32/ 4380]\n",
      "loss: 2.033500  [ 3232/ 4380]\n",
      "loss: 2.303949  [   32/ 4380]\n",
      "loss: 1.963102  [ 3232/ 4380]\n",
      "loss: 2.280403  [   32/ 4380]\n",
      "loss: 1.984793  [ 3232/ 4380]\n",
      "loss: 2.132920  [   32/ 4380]\n",
      "loss: 2.091096  [ 3232/ 4380]\n",
      "loss: 2.152683  [   32/ 4380]\n",
      "loss: 1.984625  [ 3232/ 4380]\n",
      "loss: 2.298696  [   32/ 4380]\n",
      "loss: 1.968272  [ 3232/ 4380]\n",
      "loss: 2.139597  [   32/ 4380]\n",
      "loss: 2.082088  [ 3232/ 4380]\n",
      "loss: 2.160029  [   32/ 4380]\n",
      "loss: 2.048170  [ 3232/ 4380]\n",
      "loss: 2.273214  [   32/ 4380]\n",
      "loss: 1.958649  [ 3232/ 4380]\n",
      "loss: 2.172612  [   32/ 4380]\n",
      "loss: 2.012734  [ 3232/ 4380]\n",
      "loss: 2.260595  [   32/ 4380]\n",
      "loss: 2.029854  [ 3232/ 4380]\n",
      "loss: 2.266160  [   32/ 4380]\n",
      "loss: 2.044525  [ 3232/ 4380]\n",
      "loss: 2.284700  [   32/ 4380]\n",
      "loss: 2.020473  [ 3232/ 4380]\n",
      "loss: 2.275344  [   32/ 4380]\n",
      "loss: 2.025504  [ 3232/ 4380]\n",
      "loss: 2.271000  [   32/ 4380]\n",
      "loss: 2.050093  [ 3232/ 4380]\n",
      "loss: 2.238449  [   32/ 4380]\n",
      "loss: 1.949651  [ 3232/ 4380]\n",
      "loss: 2.280570  [   32/ 4380]\n",
      "loss: 1.990056  [ 3232/ 4380]\n",
      "loss: 2.147132  [   32/ 4380]\n",
      "loss: 1.916683  [ 3232/ 4380]\n",
      "loss: 2.325911  [   32/ 4380]\n",
      "loss: 1.982242  [ 3232/ 4380]\n",
      "loss: 2.241209  [   32/ 4380]\n",
      "loss: 1.947093  [ 3232/ 4380]\n",
      "loss: 2.214407  [   32/ 4380]\n",
      "loss: 2.012629  [ 3232/ 4380]\n",
      "loss: 2.218019  [   32/ 4380]\n",
      "loss: 1.907488  [ 3232/ 4380]\n",
      "loss: 2.154561  [   32/ 4380]\n",
      "loss: 1.934642  [ 3232/ 4380]\n",
      "loss: 2.194034  [   32/ 4380]\n",
      "loss: 1.949977  [ 3232/ 4380]\n",
      "loss: 2.189063  [   32/ 4380]\n",
      "loss: 1.923081  [ 3232/ 4380]\n",
      "loss: 2.224613  [   32/ 4380]\n",
      "loss: 1.995463  [ 3232/ 4380]\n",
      "loss: 2.180408  [   32/ 4380]\n",
      "loss: 2.077460  [ 3232/ 4380]\n",
      "loss: 2.232469  [   32/ 4380]\n",
      "loss: 2.009622  [ 3232/ 4380]\n",
      "loss: 2.224045  [   32/ 4380]\n",
      "loss: 1.933553  [ 3232/ 4380]\n",
      "loss: 2.294163  [   32/ 4380]\n",
      "loss: 1.858031  [ 3232/ 4380]\n",
      "loss: 2.315997  [   32/ 4380]\n",
      "loss: 1.858320  [ 3232/ 4380]\n",
      "loss: 2.267520  [   32/ 4380]\n",
      "loss: 1.938678  [ 3232/ 4380]\n",
      "loss: 2.424287  [   32/ 4380]\n",
      "loss: 1.912862  [ 3232/ 4380]\n",
      "loss: 2.221959  [   32/ 4380]\n",
      "loss: 2.041353  [ 3232/ 4380]\n",
      "loss: 2.319379  [   32/ 4380]\n",
      "loss: 1.881737  [ 3232/ 4380]\n",
      "loss: 2.470894  [   32/ 4380]\n",
      "loss: 1.809545  [ 3232/ 4380]\n",
      "loss: 2.327350  [   32/ 4380]\n",
      "loss: 1.917197  [ 3232/ 4380]\n",
      "loss: 2.330225  [   32/ 4380]\n",
      "loss: 2.020205  [ 3232/ 4380]\n",
      "loss: 2.361245  [   32/ 4380]\n",
      "loss: 1.959955  [ 3232/ 4380]\n",
      "loss: 2.488489  [   32/ 4380]\n",
      "loss: 1.825279  [ 3232/ 4380]\n",
      "loss: 2.295277  [   32/ 4380]\n",
      "loss: 1.779434  [ 3232/ 4380]\n",
      "loss: 2.351001  [   32/ 4380]\n",
      "loss: 1.880938  [ 3232/ 4380]\n",
      "loss: 2.283111  [   32/ 4380]\n",
      "loss: 2.000534  [ 3232/ 4380]\n",
      "loss: 2.391211  [   32/ 4380]\n",
      "loss: 1.917115  [ 3232/ 4380]\n",
      "loss: 2.277178  [   32/ 4380]\n",
      "loss: 1.807682  [ 3232/ 4380]\n",
      "loss: 2.457394  [   32/ 4380]\n",
      "loss: 2.095161  [ 3232/ 4380]\n",
      "loss: 2.276079  [   32/ 4380]\n",
      "loss: 2.007226  [ 3232/ 4380]\n",
      "loss: 2.558140  [   32/ 4380]\n",
      "loss: 1.898457  [ 3232/ 4380]\n",
      "loss: 2.242275  [   32/ 4380]\n",
      "loss: 1.805657  [ 3232/ 4380]\n",
      "loss: 2.411172  [   32/ 4380]\n",
      "loss: 1.944294  [ 3232/ 4380]\n",
      "loss: 2.413091  [   32/ 4380]\n",
      "loss: 1.921201  [ 3232/ 4380]\n",
      "loss: 2.340194  [   32/ 4380]\n",
      "loss: 1.935714  [ 3232/ 4380]\n",
      "loss: 2.174946  [   32/ 4380]\n",
      "loss: 1.948885  [ 3232/ 4380]\n",
      "loss: 2.254895  [   32/ 4380]\n",
      "loss: 1.928197  [ 3232/ 4380]\n",
      "loss: 2.438005  [   32/ 4380]\n",
      "loss: 1.868496  [ 3232/ 4380]\n",
      "loss: 2.486074  [   32/ 4380]\n",
      "loss: 1.875887  [ 3232/ 4380]\n",
      "loss: 2.173772  [   32/ 4380]\n",
      "loss: 1.937173  [ 3232/ 4380]\n",
      "loss: 2.354938  [   32/ 4380]\n",
      "loss: 1.891873  [ 3232/ 4380]\n",
      "loss: 2.379659  [   32/ 4380]\n",
      "loss: 1.902722  [ 3232/ 4380]\n",
      "loss: 2.311788  [   32/ 4380]\n",
      "loss: 1.928647  [ 3232/ 4380]\n",
      "loss: 2.192856  [   32/ 4380]\n",
      "loss: 1.968259  [ 3232/ 4380]\n",
      "loss: 2.395320  [   32/ 4380]\n",
      "loss: 1.878577  [ 3232/ 4380]\n",
      "loss: 2.244478  [   32/ 4380]\n",
      "loss: 1.944192  [ 3232/ 4380]\n",
      "loss: 2.317282  [   32/ 4380]\n",
      "loss: 1.849057  [ 3232/ 4380]\n",
      "loss: 2.142006  [   32/ 4380]\n",
      "loss: 2.003831  [ 3232/ 4380]\n",
      "loss: 2.187238  [   32/ 4380]\n",
      "loss: 1.917621  [ 3232/ 4380]\n",
      "loss: 2.251067  [   32/ 4380]\n",
      "loss: 1.872128  [ 3232/ 4380]\n",
      "loss: 2.182778  [   32/ 4380]\n",
      "loss: 1.837359  [ 3232/ 4380]\n",
      "loss: 2.111881  [   32/ 4380]\n",
      "loss: 1.818405  [ 3232/ 4380]\n",
      "loss: 2.234733  [   32/ 4380]\n",
      "loss: 1.886696  [ 3232/ 4380]\n",
      "loss: 2.042076  [   32/ 4380]\n",
      "loss: 1.833245  [ 3232/ 4380]\n",
      "loss: 2.125906  [   32/ 4380]\n",
      "loss: 1.905706  [ 3232/ 4380]\n",
      "loss: 2.170933  [   32/ 4380]\n",
      "loss: 1.828883  [ 3232/ 4380]\n",
      "loss: 2.300633  [   32/ 4380]\n",
      "loss: 1.826004  [ 3232/ 4380]\n",
      "loss: 2.085946  [   32/ 4380]\n",
      "loss: 1.835575  [ 3232/ 4380]\n",
      "loss: 2.074405  [   32/ 4380]\n",
      "loss: 1.753141  [ 3232/ 4380]\n",
      "loss: 2.162486  [   32/ 4380]\n",
      "loss: 1.737031  [ 3232/ 4380]\n",
      "loss: 2.131472  [   32/ 4380]\n",
      "loss: 1.835569  [ 3232/ 4380]\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "net = SimpleNet(n = 4, m = 5, h=100, l=34).to(device)\n",
    "print(next(net.parameters()).device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "dataset = NameDataset('persianNames.txt', persian_chars_list, block_size=4,device=device)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=32)\n",
    "#print(x_hot)\n",
    "# Training loop\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    train_loop(dataloader, net, criterion, optimizer)\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dicts(chars_list):\n",
    "        char_int = {}\n",
    "        int_char = {}\n",
    "        n = len(chars_list)\n",
    "        for i in range(n):\n",
    "            char_int[chars_list[i]] = i + 1\n",
    "            int_char[i + 1] = chars_list[i]\n",
    "        char_int['.'] = 0\n",
    "        int_char[0] = '.'\n",
    "        char_int['\\u200c'] = n+1\n",
    "        int_char[n+1] = '\\u200c'\n",
    "        return (char_int, int_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_int, int_char = generate_dicts(persian_chars_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_char = char_int['ک']\n",
    "second_char = char_int['ب']\n",
    "tensor_context = F.one_hot(torch.tensor([0,0,first_char, second_char]), 34).to(device=device).float()\n",
    "sequence = [torch.tensor(first_char), torch.tensor(second_char)]\n",
    "\n",
    "while True:\n",
    "    i_next_char = torch.argmax(net(tensor_context))\n",
    "    if i_next_char == 0:\n",
    "        break\n",
    "    sequence.append(i_next_char)\n",
    "    tensor_context = torch.cat((tensor_context[1:], (F.one_hot(i_next_char, 34).to(device=device).float()).unsqueeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(26),\n",
       " tensor(3),\n",
       " tensor(2, device='cuda:0'),\n",
       " tensor(14, device='cuda:0')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"\"\n",
    "for x in sequence:\n",
    "    str += int_char[x.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'کباز'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_chars_list = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net2 = SimpleNet(n = 4, m = 5, h=100, l=28).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.01)\n",
    "dataset2 = NameDataset('names.txt', eng_chars_list, block_size=4,device=device)\n",
    "dataloader2 = DataLoader(dataset=dataset2, batch_size=32)\n",
    "#print(x_hot)\n",
    "# Training loop\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    train_loop(dataloader2, net2, criterion, optimizer)\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_int, int_char = generate_dicts(eng_chars_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0, '\\u200c': 27}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.', 27: '\\u200c'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(char_int)\n",
    "print(int_char)\n",
    "len((char_int.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_char = char_int['a']\n",
    "second_char = char_int['b']\n",
    "tensor_context = F.one_hot(torch.tensor([0,0,first_char, second_char]), 27).to(device=device).float()\n",
    "sequence = [torch.tensor(first_char), torch.tensor(second_char)]\n",
    "\n",
    "while True:\n",
    "    i_next_char = torch.argmax(net2(tensor_context))\n",
    "    if i_next_char == 0:\n",
    "        break\n",
    "    sequence.append(i_next_char)\n",
    "    tensor_context = torch.cat((tensor_context[1:], (F.one_hot(i_next_char, 27).to(device=device).float()).unsqueeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abar\n"
     ]
    }
   ],
   "source": [
    "str = \"\"\n",
    "for x in sequence:\n",
    "    str += int_char[x.item()]\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
