{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {i:[i] for i in range(256)}\n",
    "        self.encode_dict = {}\n",
    "        self.vocab_size = 256\n",
    "\n",
    "    def _most_frequent(self, byt):\n",
    "        temp = {}\n",
    "        most_frequent_pair = None        \n",
    "\n",
    "        for pair in zip(byt, byt[1:]):\n",
    "            temp[pair] = temp.get(pair, 0) + 1            \n",
    "            if most_frequent_pair == None or temp[pair] > temp[most_frequent_pair]:\n",
    "                most_frequent_pair = pair\n",
    "        \n",
    "        \n",
    "\n",
    "        return most_frequent_pair, temp[most_frequent_pair]\n",
    "    \n",
    "    def _replace(self, text, pair, pair_id):\n",
    "        new_text = []        \n",
    "\n",
    "        f = False\n",
    "        for i in range(len(text) - 1):\n",
    "            if f:\n",
    "                f = False\n",
    "                continue\n",
    "\n",
    "            if text[i] == pair[0] and text[i+1] == pair[1]:\n",
    "                new_text.append(pair_id)\n",
    "                f = True\n",
    "                continue\n",
    "\n",
    "            new_text.append(text[i])\n",
    "        \n",
    "        return new_text\n",
    "\n",
    "    def train(self, text, threshold):        \n",
    "        ids = list(text.encode())\n",
    "        count = 10**8\n",
    "\n",
    "        while count > threshold:\n",
    "            pair, count = self._most_frequent(ids)\n",
    "            # print(count)\n",
    "            self.vocab[self.vocab_size] = pair\n",
    "            self.encode_dict[pair] = self.vocab_size\n",
    "            ids = self._replace(ids, pair, self.vocab_size)\n",
    "            # print(self.vocab_size)\n",
    "            # print(ids)\n",
    "            self.vocab_size += 1\n",
    "\n",
    "\n",
    "    def find_and_replace(self, text):\n",
    "        encoded = []\n",
    "\n",
    "        i=0\n",
    "        f = False\n",
    "        while i < len(text)-1:\n",
    "            pair = text[i], text[i+1]\n",
    "            # print(pair)\n",
    "            if pair in self.encode_dict:\n",
    "                f = True\n",
    "                encoded.append(self.encode_dict[pair])\n",
    "                i+=2\n",
    "                continue\n",
    "            encoded.append(text[i])\n",
    "            i+=1\n",
    "        return f, encoded\n",
    "    \n",
    "    \n",
    "    def encode(self, text, string=False):\n",
    "        \n",
    "        if string:\n",
    "            text = list(text.encode())\n",
    "\n",
    "        f, encoded = self.find_and_replace(text)\n",
    "\n",
    "        if not f:\n",
    "            return encoded\n",
    "        \n",
    "        return self.encode(encoded)\n",
    "    \n",
    "\n",
    "    def find_and_decode(self, ids):\n",
    "        decoded = []\n",
    "\n",
    "        f = False\n",
    "        for id in ids:\n",
    "            if id > 255:\n",
    "                f = True\n",
    "                # print(\"hereee\")\n",
    "                # print(self.vocab[id])\n",
    "                decoded.append(self.vocab[id][0])\n",
    "                decoded.append(self.vocab[id][1])\n",
    "                continue\n",
    "            decoded.append(id)\n",
    "        return f, decoded\n",
    "\n",
    "    def decode(self, ids):\n",
    "        f, decoded = self.find_and_decode(ids)\n",
    "\n",
    "        if not f:\n",
    "            return ''.join(bytes(decoded).decode())\n",
    "            # return decoded\n",
    "        \n",
    "        return self.decode(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer()\n",
    "s = \"SS stands for Schutzstaffel, which was a Nazi security and military organization. Adolf Hitler established the SS in April 1925 as a personal guard unit. The SS was responsible for many crimes against humanity and war crimes during World War II, including:\"\n",
    "tokenizer.train(s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(s,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[288, 261, 100, 269, 280, 83, 270, 117, 116, 122]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(s,True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SS stands for Schutzstaffel, which was a Nazi security and military organization. Adolf Hitler established the SS in April 1925 as a personal guard unit. The SS was responsible for many crimes against humanity and war crimes during World War II, inclu'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(s,True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
